{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab6a771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Case Study - Minimizing Data Center Energy Consumption Costs\n",
    "#Artificial Intelligence Training\n",
    "\n",
    "#libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import Enviroment\n",
    "import Brain\n",
    "import DQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4522f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed Configuration for Reproducibility\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)\n",
    "\n",
    "#PARAMETER SETTING\n",
    "epsilon = .3\n",
    "number_actions = 5\n",
    "direction_boundary = (number_actions - 1) / 2\n",
    "number_epochs = 100\n",
    "max_memory = 3000\n",
    "batch_size = 512\n",
    "temperature_step = 1.5\n",
    "\n",
    "# ENVIRONMENT CREATION\n",
    "env = environment.Environment(optimal_temperature = (18.0, 24.0), initial_month = 0,\n",
    "                              initial_number_users = 20, initial_rate_data = 30)\n",
    "\n",
    "# CREATION OF THE BRAIN (NEURAL NETWORK)\n",
    "brain = brain.Brain(learning_rate = 0.00001, number_actions = number_actions)\n",
    "\n",
    "# CREATION OF DEEP Q-LEARNING\n",
    "dqn = dqn.DQN(max_memory = max_memory, discount = 0.9)\n",
    "\n",
    "# SETTING THE TRAINING/TEST MODE\n",
    "train = True\n",
    "\n",
    "# ARTIFICIAL INTELLIGENCE TRAINING\n",
    "env.train = train\n",
    "model = brain.model\n",
    "early_stopping = True\n",
    "patience = 10\n",
    "best_total_reward = -np.inf\n",
    "patience_count = 0\n",
    "if (env.train):\n",
    "    # STARTING THE LOOP THAT WILL RUN THROUGH ALL SEASONS (1 SEASON = 5 MONTHS)\n",
    "    for epoch in range(1, number_epochs):\n",
    "        # INITIALIZING ENVIRONMENT AND LOOP VARIABLES\n",
    "        total_reward = 0\n",
    "        loss = 0.\n",
    "        new_month = np.random.randint(0, 12)\n",
    "        env.reset(new_month = new_month)\n",
    "        game_over = False\n",
    "        current_state, _, _ = env.observe()\n",
    "        timestep = 0\n",
    "        \n",
    "        # STARTING THE LOOP THAT WILL CYCLE THROUGH EACH TIMESTEP (1 TIMESTEP = 1 MINUTE)\n",
    "        while ((not game_over) and timestep <= 5 * 30 * 24 * 60):\n",
    "            # EXECUTION OF THE NEXT ACTION WITH EXPLOITATION\n",
    "            if np.random.rand() <= epsilon:\n",
    "                action = np.random.randint(0, number_actions)\n",
    "                if (action - direction_boundary < 0):\n",
    "                    direction = -1\n",
    "                else:\n",
    "                    direction = 1\n",
    "                energy_ai = abs(action - direction_boundary) * temperature_step\n",
    "            \n",
    "            # EXECUTION OF THE NEXT ACTION BY INFERENCE, USING THE NEURAL NETWORK\n",
    "            else:\n",
    "                q_values = model.predict(current_state)\n",
    "                action = np.argmax(q_values[0])\n",
    "                if (action - direction_boundary < 0):\n",
    "                    direction = -1\n",
    "                else:\n",
    "                    direction = 1\n",
    "                energy_ai = abs(action - direction_boundary) * temperature_step\n",
    "            \n",
    "            # UPDATING THE ENVIRONMENT AND GETTING THE NEW STATE\n",
    "            next_state, reward, game_over = env.update_env(direction, energy_ai, int(timestep / (30 * 24 * 60)))\n",
    "            total_reward += reward\n",
    "            \n",
    "            # STORING THE NEW TRANSACTION IN MEMORY\n",
    "            dqn.remember([current_state, action, reward, next_state], game_over)\n",
    "            \n",
    "            # SEPARATION INTO TWO SEPARATE BATCHES (INPUTS AND TARGETS)\n",
    "            inputs, targets = dqn.get_batch(model, batch_size = batch_size)\n",
    "            \n",
    "            # LOSS CALCULATION USING BATCHES\n",
    "            loss += model.train_on_batch(inputs, targets)\n",
    "            timestep += 1\n",
    "            current_state = next_state\n",
    "        \n",
    "        # PRINTOUT OF THE TRAINING RESULTS FOR EACH SEASON\n",
    "        print('\\n')\n",
    "        print(\"Epoch: {:03d}/{:03d}\".format(epoch, number_epochs))\n",
    "        print(\"Total energy spent with AI: {:.0f}\".format(env.total_energy_ai))\n",
    "        print(\"Total energy spent with no AI: {:.0f}\".format(env.total_energy_noai))        \n",
    "        \n",
    "        # EARLY STOPPING\n",
    "        if (early_stopping):\n",
    "            if (total_reward <= best_total_reward):\n",
    "                patience_count += 1\n",
    "            elif (total_reward > best_total_reward):\n",
    "                best_total_reward = total_reward\n",
    "                patience_count = 0\n",
    "            if (patience_count > patience):\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "        \n",
    "        # SAVE THE MODEL\n",
    "        model.save(\"model.h5\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
